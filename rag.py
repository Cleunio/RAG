# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V2iLwjT-MOEkaw6vAgjUF_3_pani0eN_
"""

!pip install -q transformers einops accelerate bitsandbytes

!pip install -q langchain community langchain-huggingface langchainhub langchain_chroma

!pip install numpy --upgrade
!pip install transformers --upgrade

import torch
import os
import getpass

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from langchain_huggingface import HuggingFacePipeline

from langchain.prompts import PromptTemplate
from langchain_core.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain_core.messages import SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

os.environ["HF_TOKEN"] = getpass.getpass()

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained(model_id)

pipe = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    temperature=0.1,
    max_new_tokens=500,
    do_sample=True,
    repetition_penalty=1.1,
    return_full_text=False,
)
llm = HuggingFacePipeline(pipeline=pipe)

# PHI 3
#template = """
#<|system|>
#Você é um assistente virtual prestativo e está respondendo perguntas gerais. <|end|>
#<|user|>
#{pergunta}<|end|>
#<|assistant|>
#"""

# LLAMA 3
template = """
<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>
Você é um assistente virtual prestativo e está respondendo perguntas gerais.
<|eot_id|>
<|start_header_id|>user<|end_header_id|>
{pergunta}
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
"""

template

prompt = PromptTemplate.from_template(template)
prompt

chain = prompt | llm

chain.invoke({"question": "Who is the president of Brazil?"})

template_rag = """
<|begin_of_text|>
<|start_header_id|>system<|end_header_id|>
You are a helpful virtual assistant answering general questions.
Use the following retrieved context pieces to answer the question.
If you don't know the answer, just say you don't know. Keep the answer concise.
<|eot_id|>
<|start_header_id|>user<|end_header_id|>
Question: {Question}
Context: {Context}
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
"""

prompt_rag = PromptTemplate.from_template(template_rag)
print(prompt_rag)

from datetime import date
dia = date.today()
print(dia)

context = "you know that the date today is ".format(dia)
print(context)

chain_rag = prompt_rag | llm | StrOutputParser()
question = "What is the date today? Answer the date in the dd/mm/yyyy format"
chain_rag.invoke({"Question": question, "Context": context})

chain_rag2 = prompt_rag | llm | StrOutputParser()
context2 = """Quarterly Revenue:
1º: R$42476,40
2º: R$46212,97
3º: R$41324,56
4º: R$56430,24"""

question2 = "What is the quarterly revenue of the company?"
chain_rag2.invoke({"context2": context2, "question2": question2})